{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv1sOQg5zV0E"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    sentence-transformers \\\n",
        "    faiss-cpu \\\n",
        "    pymupdf \\\n",
        "    pydantic \\\n",
        "    streamlit \\\n",
        "    networkx \\\n",
        "    scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqeDtUc1zcLu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import fitz\n",
        "import uuid\n",
        "import networkx as nx\n",
        "from typing import List, Dict\n",
        "from collections import Counter\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO4xqQ9szoq1"
      },
      "outputs": [],
      "source": [
        "class PaperSection(BaseModel):\n",
        "    name: str\n",
        "    text: str\n",
        "\n",
        "class ResearchPaper(BaseModel):\n",
        "    paper_id: str\n",
        "    title: str\n",
        "    authors: List[str]\n",
        "    year: int\n",
        "    abstract: str\n",
        "    sections: List[PaperSection]\n",
        "    keywords: List[str]\n",
        "    references: List[str]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4JHUqXX0PrS"
      },
      "outputs": [],
      "source": [
        "SECTION_MARKERS = [\n",
        "    \"abstract\", \"introduction\", \"method\", \"methodology\",\n",
        "    \"experiment\", \"results\", \"discussion\", \"conclusion\", \"references\"\n",
        "]\n",
        "\n",
        "def extract_sections_from_pdf(pdf_path: str) -> Dict[str, str]:\n",
        "    doc = fitz.open(pdf_path)\n",
        "    raw_text = \" \".join([page.get_text() for page in doc])\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", raw_text).lower()\n",
        "\n",
        "    extracted = {}\n",
        "    for i, sec in enumerate(SECTION_MARKERS):\n",
        "        start = text.find(sec)\n",
        "        if start == -1:\n",
        "            continue\n",
        "        end = len(text)\n",
        "        for nxt in SECTION_MARKERS[i+1:]:\n",
        "            idx = text.find(nxt, start + 20)\n",
        "            if idx != -1:\n",
        "                end = idx\n",
        "                break\n",
        "        extracted[sec] = text[start:end].strip()\n",
        "\n",
        "    return extracted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lQeFazl0U3i"
      },
      "outputs": [],
      "source": [
        "def build_research_paper(pdf_path: str) -> ResearchPaper:\n",
        "    sections = extract_sections_from_pdf(pdf_path)\n",
        "\n",
        "    paper = ResearchPaper(\n",
        "        paper_id=str(uuid.uuid4()),\n",
        "        title=os.path.basename(pdf_path).replace(\".pdf\", \"\"),\n",
        "        authors=[\"Unknown\"],\n",
        "        year=2023,\n",
        "        abstract=sections.get(\"abstract\", \"\")[:1500],\n",
        "        sections=[\n",
        "            PaperSection(name=k, text=v)\n",
        "            for k, v in sections.items()\n",
        "            if k not in [\"abstract\", \"references\"]\n",
        "        ],\n",
        "        keywords=[\"AI\", \"Deep Learning\"],\n",
        "        references=sections.get(\"references\", \"\").split(\"\\n\")[:20]\n",
        "    )\n",
        "    return paper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQu-0dwx0Zp8"
      },
      "outputs": [],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=600,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "def create_chunks(paper: ResearchPaper) -> List[Document]:\n",
        "    docs = []\n",
        "    for sec in paper.sections:\n",
        "        chunks = splitter.split_text(sec.text)\n",
        "        for ch in chunks:\n",
        "            docs.append(\n",
        "                Document(\n",
        "                    page_content=ch,\n",
        "                    metadata={\n",
        "                        \"paper_id\": paper.paper_id,\n",
        "                        \"section\": sec.name,\n",
        "                        \"year\": paper.year,\n",
        "                        \"title\": paper.title\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "    return docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQXPW8Es0ed2"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "def embed_texts(texts):\n",
        "    return embedding_model.encode(texts)\n",
        "\n",
        "def build_faiss_index(documents: List[Document]):\n",
        "    texts = [d.page_content for d in documents]\n",
        "    metadatas = [d.metadata for d in documents]\n",
        "    return FAISS.from_texts(texts, embedding_model, metadatas=metadatas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HHWF7zy0iDO"
      },
      "outputs": [],
      "source": [
        "def semantic_search(query, vectorstore, top_k=5):\n",
        "    return vectorstore.similarity_search(query, k=top_k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-_RaZdU0n78"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.llms import OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\" # Replace with your actual OpenAI API Key\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "def rag_answer(query, vectorstore):\n",
        "    docs = semantic_search(query, vectorstore)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Answer strictly using the context below.\n",
        "If not found, say \"Not mentioned in papers\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\"\"\"\n",
        "    return llm(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRUG0R4T0qpv"
      },
      "outputs": [],
      "source": [
        "def compare_papers(query, vectorstore):\n",
        "    docs = semantic_search(query, vectorstore, top_k=8)\n",
        "    combined = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Compare the following research excerpts:\n",
        "\n",
        "{combined}\n",
        "\n",
        "Provide differences in methods and contributions.\n",
        "\"\"\"\n",
        "    return llm(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZWB5CFA1HLI"
      },
      "outputs": [],
      "source": [
        "def create_citation_graph(papers: List[ResearchPaper]):\n",
        "    graph = nx.DiGraph()\n",
        "    for p in papers:\n",
        "        for ref in p.references:\n",
        "            graph.add_edge(p.title, ref[:80])\n",
        "    return graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srQFP6_J1Kk1"
      },
      "outputs": [],
      "source": [
        "def trend_analysis(papers: List[ResearchPaper]):\n",
        "    yearly_keywords = {}\n",
        "    for p in papers:\n",
        "        yearly_keywords.setdefault(p.year, []).extend(p.keywords)\n",
        "\n",
        "    trends = {\n",
        "        year: Counter(words).most_common(5)\n",
        "        for year, words in yearly_keywords.items()\n",
        "    }\n",
        "    return trends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEdZpOsn1M9k"
      },
      "outputs": [],
      "source": [
        "def paper_metadata_tool(title):\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"venue\": \"arXiv\",\n",
        "        \"year\": 2023,\n",
        "        \"citations\": 100\n",
        "    }\n",
        "\n",
        "def related_work_tool(query, vectorstore):\n",
        "    return semantic_search(query, vectorstore, top_k=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuT-WFQX1QRf"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\" Research Intelligence Assistant\")\n",
        "\n",
        "query = st.text_input(\"Ask a research question\")\n",
        "\n",
        "if query:\n",
        "    st.write(\"Answer will be generated from indexed papers.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tK99UQWO1Xz2"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "st.set_page_config(page_title=\"Research Intelligence System\", layout=\"wide\")\n",
        "\n",
        "# ---------------- EMBEDDING ----------------\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ---------------- PDF PARSER ----------------\n",
        "SECTIONS = [\"abstract\", \"introduction\", \"method\", \"results\", \"conclusion\", \"references\"]\n",
        "\n",
        "def parse_pdf(pdf):\n",
        "    doc = fitz.open(stream=pdf.read(), filetype=\"pdf\")\n",
        "    text = \" \".join(p.get_text() for p in doc).lower()\n",
        "    parsed = {}\n",
        "\n",
        "    for i, sec in enumerate(SECTIONS):\n",
        "        start = text.find(sec)\n",
        "        if start == -1:\n",
        "            continue\n",
        "        end = len(text)\n",
        "        for nxt in SECTIONS[i+1:]:\n",
        "            idx = text.find(nxt, start+50)\n",
        "            if idx != -1:\n",
        "                end = idx\n",
        "                break\n",
        "        parsed[sec] = text[start:end]\n",
        "    return parsed\n",
        "\n",
        "# ---------------- CHUNKING ----------------\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=80)\n",
        "\n",
        "def create_docs(parsed, title):\n",
        "    docs = []\n",
        "    for sec, txt in parsed.items():\n",
        "        for chunk in splitter.split_text(txt):\n",
        "            docs.append(\n",
        "                Document(\n",
        "                    page_content=chunk,\n",
        "                    metadata={\"section\": sec, \"title\": title}\n",
        "                )\n",
        "            )\n",
        "    return docs\n",
        "\n",
        "# ---------------- UI ----------------\n",
        "st.title(\"üìö Research Paper Intelligence Assistant\")\n",
        "\n",
        "uploaded = st.file_uploader(\"Upload Research Paper (PDF)\", type=[\"pdf\"])\n",
        "\n",
        "if uploaded:\n",
        "    with st.spinner(\"Processing PDF...\"):\n",
        "        parsed = parse_pdf(uploaded)\n",
        "        docs = create_docs(parsed, uploaded.name)\n",
        "        texts = [d.page_content for d in docs]\n",
        "        metadatas = [d.metadata for d in docs]\n",
        "        vectorstore = FAISS.from_texts(texts, embedder, metadatas=metadatas)\n",
        "\n",
        "    st.success(\"Paper Indexed Successfully!\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        st.subheader(\"üîç Semantic Search\")\n",
        "        q = st.text_input(\"Ask a question\")\n",
        "        if q:\n",
        "            results = vectorstore.similarity_search(q, k=4)\n",
        "            for r in results:\n",
        "                st.markdown(f\"**Section:** {r.metadata['section']}\")\n",
        "                st.write(r.page_content[:600])\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"üìà Keyword Trends\")\n",
        "        words = []\n",
        "        for d in docs:\n",
        "            words.extend(d.page_content.split())\n",
        "        trends = Counter(words).most_common(10)\n",
        "        st.table(trends)\n",
        "\n",
        "else:\n",
        "    st.info(\"Upload a PDF to start\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQHzwrnz_OAk"
      },
      "outputs": [],
      "source": [
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TncypX1iXUZ",
        "outputId": "4e58f994-e58e-41a3-b41c-ff435e9930a6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0Kyour url is: https://vast-meals-own.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "-atuyv7ti1yo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}